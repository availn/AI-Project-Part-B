/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 
 *            COMP30024 Artificial Intelligence - Semester 1 2016            *
 *                  Project B - Playing a Game of Hexifence                  *
 *                                                                           *
 *                               Comments.txt                                *
 *                                                                           *
 *    Submission by: Julian Tran <juliant1> and Matt Farrugia <farrugiam>    *
 *                  Last Modified 17/05/16 by Matt Farrugia                  *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

Marks:

    4 - quality of code and comments
    4 - correctness and reliability
    7 - results of testing
    7 - creativity of solution
	
Introduction

    Welcome to our submission. Here you will find a series of Java classes built for playing the best game of Hexifence you have seen in your life! Our solution is broken into a number of packages, and an even larger number of classes (spanning over 4000 lines of code in total! In this comments.txt file, we'll briefly discuss the structure of those packages and classes, before diving into a report on our Agent's winning game strategy.

    We've managed to develop an Agent that can, in a reasonable time and without using much memory, search all the way to a terminal state from near the beginning of a dimension 2 game. Obviously we're not enumerating all possibilities in this search; we've used a host of domain-specific heuristics and approximations to lower our branching factor. As a result, there are several limitations to our Agent's strategy, which we have documented as well. However, I think we can proudly declare that this Agent's Intelligence is a product of our own, truly representing our mastery of the game (though with a much faster clock speed!)

    In addition to employing clever heuristics, our Agent employs two purpose-built data structures on top of its board representation, for improving time complexity where it counts. We've also been able to pursue the design of this agent with more clarity than most, by leveraging LibGDX (a Java Game Development Library) to render Hexifence Boards in colourful 2D -- so we can watch our Agent dominate opponents without straining to interpret ASCII symbols on the command line. We've included some sample .jar files so that you can see this for yourself, and written more about it later in this report.

    We hope you can take the time to enjoy reading about the fun we have had with this project.

                                                                Julian and Matt

Structure of Solution

    Our solution became a bit expansive as we extended our simple agents and board representation into its final form. It's made up of 8 packages; listed here in rough order of relevance;

    - unimelb.farrugiulian.hexifence.agent
        Here are our game-playing Agents:
        - Agent
            A superclass of all Hexifence Agents, legally playing a game according to the Player interface, but delegating all decision-making to its subclass' 'getChoice()' method. Made to save on code duplication between Agents.
        - AgentFarrugiulian
            Our final Agent submission, more about its strategy later...
        - AgentGreedy
            An experimental AI playing the game with a greedy strategy - making moves that capture cells if possible, otherwise minimising the immediate gain of the opponent
        - AgentMe
            An experimental AI that forwards all of its decisions to a human player by reading instructions from stdin
        - AgentRandom
            An experimental AI that selects random legal moves
        - DoubleAgent
            An experimental AI that plays the game with no searching, just using a handful of hard-coded strategies to choose the best visible move based on the information available

    - unimelb.farrugiulian.hexifence.board
        This package contains our data structure implementing the game board, made to abstract away logic related to navigating the game grid (instead considering it a collection of cells surrounded by edges, in a more graph-like manner). See the javadoc for more information on these classes and their provided functionality.

    - unimelb.farrugiulian.hexifence.board.features
        This class provides another layer of abstraction above the game board, allowing it to be viewed by the Agent as either a collection of available Edges with certain classifications (based on the immediate consequences of choosing those edges) and as a collection of 'Features', collections of edges that represent 'chains' or 'loops' of one or more neighbouring cells that can all be captured if one of them is captured; and 'intersections' where these features meet. We'll talk more about these data structures later, too.

    - com.matomatical.hexifence.visual
        Specialised Referee and Interfaces for interacting with the Hexifence Visualiser (that we built using LibGDX)

    - com.matomatical.util
        For extensions of standard Java utility classes, namely the QueueHashSet extension of Java's LinkedHashSet (more about data structures later)

    - aiproj.hexifence
        Provided Referee class, Player interface, etc, unchanged since copied over from dimefox on 16/05/16.
    
    - aiproj.hexifence.matt
        Our own implementation of a Hexifence Referee, for experimental purposes.
    
    - unimelb.farrugiulian.hexifence.board.boardreading
        Some leftover classes adapted from Project Part A: for reading and building boards based on specially-formatted ASCII input

AgentFarrugiulian
	
	Our agent divides each game up into three parts - the opening, the midgame, and the endgame. After every move is made, the agent checks if it should change its strategy. This transition check is done based on the number of "safe" edges left, or, the number of edges left that can be placed without allowing a player to score. The agent switches from its opening strategy to its midgame strategy once there are less than 30 safe edges left, and from its midgame strategy to its endgame strategy once there are no safe edges left.

	Our opening strategy is to first make any move that increases our score. If there is no such move, then we make a random safe move. We are not using a search at this point because we believe that there is limited benefit in doing so, as the search would have to be quite shallow due to the high branching factor, and we would have to construct a utility function which was capable of meaningfully evaluating boards at very early stages of the game.

	Our midgame strategy is to first make any move that increases our score. If there is no such move, then we do an approximate minimax search (with pruning) for the best safe move to make. Our search space is over all safe edges, as we could not find a situation where making an unsafe move would be beneficial, and are assuming that if such a situation exists, it would be rare enough to warrant trading a more complete search for a faster search. We consider any board state which has no safe edges to be a terminal state, and evaluate this state using a smaller minimax search on features (the chains, loops, and intersections that appear) followed by a heuristic playout. This evaluation determines which players will win or lose from this board state, with no gray area in between. This has the benefit of effectively implementing a fast alpha-beta pruning, because as soon as a player in the search expands a move which has a valuation where that player is expected to win, then that player can return that valuation immediately without expanding any further moves. This however relies on our evaluation function being correct, as an incorrect valuation can easily be returned all the way back up the search tree with this method.

	Our endgame strategy is to first make any move that both increases our score and does not create another move that can increase our score. This new condition stems from strategy seen in the more common "dots and boxes" game, where in the endgame, it is not optimal to greedily increase your score. Instead, optimal play involves "staying in control", where you force your opponent to open or "sacrifice" long chains of boxes, at which point you take all but two (or sometimes four) of the boxes, and "double box" the other two of the boxes, which forces the opponent, who is not in control, to take those few boxes, and sacrifice another long chain of boxes. Therefore, the only time we can be trivially sure that capturing a cell is a smart move, is when we know that capturing the cell does not make anotehr cell capturable.
	If there are no moves that allow for this, we can still maybe capture cells without thinking too much if we are not in a position to double box. So if we are still able to capture cells, we calculate how many cells we can capture before allowing the opponent to make their move. If this number is two, then we know we are in a position where we might want to double box, as if these two cells were capturable with the same move, then we would have made this move before doing this check. Likewise, if this number is four and one of the moves we would make in capturing these four cells would capture two cells at once, then we are also in a position where we might want to double box.
	Therefore, if we know we are in a position where we definitely do not want to double box, i.e. not in one of the aforementioned positions, then we can safely make moves that increase our score without needing to consider how the game will turn out. We only need to make sure that the edge we take does not remove the possibility that we are able to double box later on (Though the only time we should encounter this situation is when the opponent makes a very bad move).
	If we are in a position to double box, we need to determine whether it is actually a good idea to do so. We create a board state where it is as if we chose to double box (so the opponent has captured the cells we gave up and it is now their turn to move again), and then use our minimax search on features foloowed by a heuristic playout to play out the game from here, and check if we should be guaranteed the win. If so, then obviously double boxing is the right choice (though this results in our agent sometimes being quite disrespectful by double boxing as its very last move, effectively giving the opponent some score because our agent does not need it), and if not then we choose to just increase our own score.
	If there are no moves that increase our score, then we need to make a move that will allow the opponent to increase their score (making a sacrifice). Again, we use a minimax search on features followed by a heuristic playout to determine if we can be guaranteed a win from here. If so, then we make the sacrifice returned by the feature search, and if not we make the smallest sacrifice possible. How we make this sacrifice is important if we are making a sacrifice of two cells. If we are "in control" of the game, then we do not want to allow the opponent to double box our sacrifice, and take away our control, so we sacrifice the chain securely in the middle. If we are not in control, we sacrifice the chain on the edge, hoping that the opponent is not smart and double boxes it, so that we may later take control, or at the very least increase our score a little.

	The minimax search over features which has been mentioned a few times, is our way of removing a lot of the branching factor in late game. Early on we recognised that in the late game, most of the decision making comes down to gaining an odd number of short chains at the beginning of your turn, so that the opponent will be forced to open the first long chain. Therefore, a minimax search over features is done in order to determine the best way to break up forks which are connected by small chains (there are typically few of these, usually at most three or four). From here, instead of continuing to use a search to play out the game, we just assume a branching factor of one, and play out the game heuristically. We assume that all players will open sacrifices securely, and will always double if given the chance, and then we take note of the expected winner. This allows us to evaluate game quickly, while hopefully still being accurate.


Our Agent's Data Structures

    
    - Data Structures
        - EdgeSet making initial decisions take constant time to save on time
        - FeatureSet massively reduces branching factor by collecting equivalent moves towards the end of the game

Our Other Agents

    - AgentMe
    - AgentRandom
	- AgentGreedy
	AgentGreedy is an agent that will always make scoring moves when possible. If not possible, then it will make a safe move, and if this is also not possible, it will make a move that allows the opponent to gain the least amount of score. This agent was the first informed agent that we created, and making it helped us understand the sorts of features that appeared in the endgame, and got us writing functions to help quanify these features.

	- DoubleAgent
	DoubleAgent is an extension of the greedy agent, where it assumes a branching factor of one once it reaches the endgame, and plays out the game heuristically (and by it I really mean me, as I did not thinking there was actually branching happenning until Matt brought me back down to earth). It will always take edges that both score and do not create other edges that score. Failing that, it will take safe edges. And failing that, it will play out the endgame, double boxing and baiting wherever necessary in order to have an odd number of short chains at the beginning of its turn. From this agent, a lot of the endgame logic was copied to AgentFarrugiulian, but using the new FeatureSet in order to make a lot of the code more bearable.

    which all really influenced the final design of AgentFarrugiulian

Other Creative Aspects: The Hexifence Visualisation Engine

    In addition to the classes that make up our submission, as we have mentioned we also created a visualisation engine using LibGDX. We have not included the source code for this visualisation engine, but have packaged a few standalone .jar files so that you can view the results. See the `visual/` directory of our submission!

    More details on that, instructions, etc
    
Other Creative Aspects: Possible Extensions and Research and stuff

    - Monte Carlo and Nims and stuff like that
    
    - Endgame Search more and stuff like that

    - Merging the featureset and the edgeset for a more comprehensive search
    
    - Transposition table and Lexicographic Pruning (even though the latter of which turns out to be a dud!)

Fun times:

    Skype discussions and screenshots and paper and stuff (46 + 5 + 59 + 39 mins so far, plus 57 + 2:53, plus 1:28 + 1:27 + 11 + 3:34 + 46, plus 1:52 + 14 + ...)

    'after we finished talking about 4-loops, julian was struggling with for-loops'

    Independently both accidentally completing the midgame search function on the same morning.
