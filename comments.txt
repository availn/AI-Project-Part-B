/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 
 *            COMP30024 Artificial Intelligence - Semester 1 2016            *
 *                  Project B - Playing a Game of Hexifence                  *
 *                                                                           *
 *                               Comments.txt                                *
 *                                                                           *
 *    Submission by: Julian Tran <juliant1> and Matt Farrugia <farrugiam>    *
 *                  Last Modified 17/05/16 by Matt Farrugia                  *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

    Welcome to our submission. Here you will find a series of Java classes built for playing the best game of Hexifence you have seen in your life! Our solution is broken into a number of packages, and an even larger number of classes (spanning over 4000 lines of code in total! In this comments.txt file, we'll briefly discuss the structure of those packages and classes, before diving into a report on our Agent's winning game strategy.

    We've managed to develop an Agent that can, in a reasonable time and without using much memory, search all the way to a terminal state from near the beginning of a dimension 2 game. Obviously we're not enumerating all possibilities in this search; we've used a host of domain-specific heuristics and approximations to lower our branching factor. As a result, there are several limitations to our Agent's strategy, which we have documented as well. However, I think we can proudly declare that this Agent's Intelligence is a product of our own, truly representing our mastery of the game (though with a much faster clock speed!)

    In addition to employing clever heuristics, our Agent employs two purpose-built data structures on top of its board representation, for improving time complexity where it counts. We've also been able to pursue the design of this agent with more clarity than most, by leveraging LibGDX (a Java Game Development Library) to render Hexifence Boards in colourful 2D -- so we can watch our Agent dominate opponents without straining to interpret ASCII symbols on the command line. We've included some sample .jar files so that you can see this for yourself, and written more about it later in this report.

    We hope you can take the time to enjoy reading about the fun we have had with this project.
                                                                Julian and Matt

Structure of Solution

    Our solution became a bit expansive as we extended our simple agents and board representation into its final form. It's made up of 8 packages; listed here in rough order of relevance;

    - unimelb.farrugiulian.hexifence.agent
        Here are our game-playing Agents:
        - Agent
            A superclass of all Hexifence Agents, legally playing a game according to the Player interface, but delegating all decision-making to its subclass' 'getChoice()' method. Made to save on code duplication between Agents.
        - AgentFarrugiulian
            Our final Agent submission, more about its strategy later...
        - AgentGreedy
            An experimental AI playing the game with a greedy strategy - making moves that capture cells if possible, otherwise minimising the immediate gain of the opponent
        - AgentMe
            An experimental AI that forwards all of its decisions to a human player by reading instructions from stdin
        - AgentRandom
            An experimental AI that selects random legal moves
        - DoubleAgent
            An experimental AI that plays the game with no searching, just using a handful of hard-coded strategies to choose the best visible move based on the information available
        Our experimental Agents ended up helping us to build our final Agent, AgentFarrugiulian.

    - unimelb.farrugiulian.hexifence.board
        This package contains our data structure implementing the game board, made to abstract away logic related to navigating the game grid (instead considering it a collection of cells surrounded by edges, in a more graph-like manner). See the javadoc for more information on these classes and their provided functionality.

    - unimelb.farrugiulian.hexifence.board.features
        This class provides another layer of abstraction above the game board, allowing it to be viewed by the Agent as either a collection of available Edges with certain classifications (based on the immediate consequences of choosing those edges) and as a collection of 'Features', collections of edges that represent 'chains' or 'loops' of one or more neighbouring cells that can all be captured if one of them is captured; and 'intersections' where these features meet. We'll talk more about these data structures later, too.

    - com.matomatical.hexifence.visual
        Specialised Referee and Interfaces for interacting with the Hexifence Visualiser (that we built using LibGDX)

    - com.matomatical.util
        For extensions of standard Java utility classes, namely the QueueHashSet extension of Java's LinkedHashSet (more about data structures later)

    - aiproj.hexifence
        Provided Referee class, Player interface, etc, unchanged since copied over from dimefox on 16/05/16.
    
    - aiproj.hexifence.matt
        Our own implementation of a Hexifence Referee, for experimental purposes.
    
    - unimelb.farrugiulian.hexifence.board.boardreading
        Some leftover classes adapted from Project Part A: for reading and building boards based on specially-formatted ASCII input

AgentFarrugiulian
	
    Our Agent divides each game up into three stages - the opening stage, the mid-game stage, and the endgame stage. We will outline the approaches used in each stage below.

Opening Strategy

    In the opening stages of the game, our agent plays a random, safe move (unless there are cells available for capture)

    As simple as it sounds, a deal of effort went into making these turns happen as quickly as possible. It turns out creating a collection capable of supporting constant time access and removal of specific edges, and also constant time removal of a random edge, is not such a simple task.

The EdgeSet Data Structure

    Our Agent achieves constant-time random moves by maintaining the first of our specialised data structures - the EdgeSet. This data structure is also used by the mid-game search algorithm, and the effects are more noticeable there, where this complexity improvement is magnified by the exponential nature of a search. Moreover, having a data structure supporting fast, randomised but heuristic decision-making is a crucial part of an effective 'play-out' based strategy, such as a Monte-Carlo tree search, or the strategy we ended up arriving at for our final agent.

    The data structure works by maintaining four fast-access collections of empty (legal) Edges. The collections are:
        - free edges (capturing a cell)
        - capturing edges (capturing a cell and making another capture-able)
        - safe edges (don't make another cell capture-able)
        - sacrificing edges ()
    When a move is played, the EdgeSet is updated and kept accurate, considering the consequences to all nearby Edges.

    It's therefore important that we use a collection type that supports fast insertion and removal of specific elements. HashSets come to mind. However, our collections would be useless for our purposes if they did not support fast removal of elements for selection. We need to be able to pull SOMETHING out of the safe set, for example, when we are looking for a safe move to actually play. HashSets, unfortunately, only support linear time removal in this manner.

    That's where the specialised utility class comes in. The QueueHashSet is an extension of Java's native LinkedHashSet, which allows fast FIFO removal of elements by maintaining a linked list atop the hash table structure. Our QueueHashSet extends this to implement the Queue interface, providing convenient methods for extracting an item as per our requirements. So with it, we have the ability to insert, update, delete and get an element, all in O(1) time! Awesome!

    One further detail that helps speed up our game playing is the fact that the EdgeSet supports the UN-making of moves. By keeping itself up to date in both the forward and the backward directions, we are able to conduct searches on a single underlying board (by restoring it to its original position as we unwind the recursive search algorithms), saving us from having to copy the board to generate new states - search transitions can happen in constant time!

Mid-game Strategy

    We wouldn't get very far with an agent that makes random moves. Our aim during opening is actually to transition to the mid-game stage as early as possible, such that our mid-game search algorithm will not take too long. We achieved this by testing various transition thresholds and finding that when about 30 safe edges remain, our search completes in a number of seconds. If this turns out to be too early, time-outs will ensure that we default back to effectively random safe behaviour before a rouge search takes too much of our playtime.

    Our approach to searching from this point is an approximate minimax search with a highly simplified, but more effective variation of alpha-beta pruning based on accurately determining the winning player from states with no remaining safe edges.

    Our search is 'approximate', as it does not actually consider ALL possible moves from each search state. While there are still safe edges remaining, it considers only these safe edges as valid moves. It's unfortunately possible, however, that the only winning move requires making an early sacrifice of a few cells, achieved by taking an edge that is NOT a safe edge. Our search ignores these moves from both players' perspectives, and this means that our Agent is blind to unsafe, winning moves. If they happen to exist in a particular game, another Agent could defeat us even if we thought we were going to win. However, these situations seem to be difficult to imagine and rare to encounter, meaning that the time taken to consider them at every layer of our search may not be worth the branching-factor blow-out!

    Our Agent manages to complete searches beginning with between 20 and 30 safe edges within a few seconds (with the time depending on the distribution of the edges, depending on how many safe edges are eliminated by the placing of other edges in the search). A branching factor of 30 may seem unrealistically high, however in each ply the number of remaining safe moves is reduced by a number larger than 1, as most safe moves involve the reclassification of about two other moves as now unsafe. Therefore, the number of nodes in the search tree is not 30!, but closer to 30*27*24*21*...*3 (a triple factorial: https://en.wikipedia.org/wiki/Factorial#Double_factorial). Another result is that since the Agent's next turn is likely to start with a board with two safe edges having been played and therefore about 6 less safe edges, subsequent searches will have search trees of rapidly decreasing size!

    We have also included a simplified version of alpha-beta pruning in our mid-game search. This relies on us being able to accurately evaluate boards with no safe edges and determine the winning player. Since our resulting evaluation function has a domain of only two possible values, we're able to prune more aggressively; at each layer of the search tree, once we find a move that results in the player at that layer winning the game, we can safely return this as an optimal move without ever needing to evaluate any other options from this state.

    Our evaluation of boards with no safe edges is actually not a static evaluation, but a fast 'play-out' search from that board all the way to a real terminal state, using the same search strategy as our Agent uses during endgame. Using the second of our specialised data structures (discussed below), this search can be conducted with an incredibly low branching factor, despite the fact that there are likely to be a large number of legal moves remaining. This play-out is close to linear time in the size of the board in practice. We can therefore quickly and accurately determine the expected winner from one of these states, as if we were using a static evaluation function, and drive the play towards these winning states.

The FeatureSet Data Structure

    When there are no 'safe' moves remaining, it's useful to stop viewing the board as a set of legal edges, and begin to view it as a set of higher-order features; 'chains' and 'loops'. These are groups of cells that are bordered by collections of edges, and choosing any one of the edges results in all of the cells becoming available for capture one after the other.

    At this point, when edges can be naturally grouped into sets of effectively equivalent moves, a search over edges becomes pretty much useless due to its unnecessarily high branching factor. It's much better to partition the edges into their equivalence classes and search based on these.

    This is where the FeatureSet comes in. A FeatureSet is a collection of these board features (chains, loops, and the 'intersection' cells where they connect to one another). Using a linear time algorithm reminiscent of Depth-First Search, we can take a board and scan it for these Features. Then, we can conduct a search by Cloning the FeatureSet and making changes to it, all the while keeping track of the score on the board, before eventually reaching a terminal state and inspecting this score to determine who the winner was.

    Using the FeatureSet like this to dramatically reduce the branching factor in the late stages of the game is the secret to our fast and accurate mid-game evaluations. It's also the backbone of our endgame strategy, which we'll talk more about soon. However, it has more potential than in just these applications!

    Firstly, having a way to carefully analyse the board for these features is a fantastic first step to constructing a powerful static evaluation function; where the existing features could be carefully analysed to determine a utility value for a given non-terminal state. We basically decided not to go down this route since any powerful utility function would need to take into account the very complex interactions features and their counts as they play into the flow of control at the late stages of the game, for example one important feature can be the parity of the number of small sacrifices on the board as it determines the player that will be forced to open the first long chain of cells, likely losing the game if there are enough of these long features to overcome the cost of the sacrifices beforehand.

    Secondly, though not attempted for this project, a FeatureSet could be extended to keep track of a board's features before there are no safe edges remaining, in some sense 'merging' it with our EdgeSet to provide a richer analysis of the current state of the board. Though incredibly complex to implement (especially the requirement to keep it up to date with the placing and removal of edges throughout the whole game) would make a more complete mid-game and endgame search far more feasible.

Endgame Strategy

	Our endgame strategy is to first make any move that both increases our score and does not create another move that can increase our score. This new condition stems from strategy seen in the more common "dots and boxes" game, where in the endgame, it is not optimal to greedily increase your score. Instead, optimal play involves "staying in control", where you force your opponent to open or "sacrifice" long chains of boxes, at which point you take all but two of the boxes, and "double box" the other two of the boxes. This forces the opponent, to take those two boxes, and sacrifice another long chain of boxes. Therefore, the only time we can be trivially sure that capturing a cell is a smart move, is when we know that capturing the cell does not make another cell capture-able.

	If there are no moves that allow for this, we can still maybe capture cells without thinking too much if we are not in a position to double box. So if we are still able to capture cells, we calculate how many cells we can capture before allowing the opponent to make their move. If this number is two, then we know we are in a position where we might want to double box, as if these two cells were capture-able with the same move, then we would have made this move before doing this check. Likewise, if this number is four and one of the moves we would make in capturing these four cells would capture two cells at once, then we are also in a position where we might want to double box.

	Therefore, if we know we are in a position where we definitely do not want to double box, i.e. not in one of the aforementioned positions, then we can safely make moves that increase our score without needing to consider how the game will turn out. We only need to make sure that the edge we take does not remove the possibility that we are able to double box later on (Though the only time we should encounter this situation is when the opponent makes a very bad move).

	If we are in a position to double box, we need to determine whether it is actually a good idea to do so. We create a board state where it is as if we chose to double box (so the opponent has captured the cells we gave up and it is now their turn to move again), and then use our minimax search on features followed by a heuristic play-out to play out the game from here, and check if we should be guaranteed the win. If so, then obviously double boxing is the right choice (though this results in our agent sometimes being quite disrespectful by double boxing as its very last move, effectively giving the opponent some score because our agent does not need it), and if not then we choose to just increase our own score.
	If there are no moves that increase our score, then we need to make a move that will allow the opponent to increase their score (making a sacrifice). Again, we use a minimax search on features followed by a heuristic play-out to determine if we can be guaranteed a win from here. If so, then we make the sacrifice returned by the feature search, and if not we make the smallest sacrifice possible. How we make this sacrifice is important if we are making a sacrifice of two cells. If we are "in control" of the game, then we do not want to allow the opponent to double box our sacrifice, and take away our control, so we sacrifice the chain securely in the middle. If we are not in control, we sacrifice the chain on the edge, hoping that the opponent is not smart and double boxes it, so that we may later take control, or at the very least increase our score a little.

	The minimax search over features which has been mentioned a few times, is our way of removing a lot of the branching factor in late game. Early on we recognised that in the late game, most of the decision making comes down to gaining an odd number of short chains at the beginning of your turn, so that the opponent will be forced to open the first long chain. Therefore, a minimax search over features is done in order to determine the best way to break up forks which are connected by small chains (there are typically few of these, usually at most three or four). From here, instead of continuing to use a search to play out the game, we just assume a branching factor of one, and play out the game heuristically. We assume that all players will open sacrifices securely, and will always double if given the chance, and then we take note of the expected winner. This allows us to evaluate game quickly, while still being accurate.

Other Creative Aspects: The Hexifence Visualisation Engine

    In addition to the classes that make up our submission, as we have mentioned we also created a visualisation engine using LibGDX. We have not included the source code for this visualisation engine, but have packaged a few standalone .jar files so that you can view the results. Time delays have been included between moves so that the games are easier to follow. See the `visual/` directory of our submission, and have fun watching our agents play off on boards of various dimensions!
    
Other Creative Aspects: Possible Extensions and Research and stuff

    - Monte Carlo - seems to be well suited for this style of game, MC for example has performed well in the similar game of GO - a high-branching factor, impartial, stage-based and control-based game

    - Nims and Spage-Grundy theory

    - Directions for machine learning - basically a linear evaluation function just isn't gonna cut it for this game

    - Transposition table and Lexicographic Pruning (even though the latter of which turns out to be a dud!)

Fun times:

    Skype discussions and screenshots and paper and stuff (46 + 5 + 59 + 39 mins so far, plus 57 + 2:53, plus 1:28 + 1:27 + 11 + 3:34 + 46, plus 1:52 + 14 + ...)

    'after we finished talking about 4-loops, julian was struggling with for-loops'

    Independently both accidentally completing the midgame search function on the same morning.
